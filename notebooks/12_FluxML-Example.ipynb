{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FluxML Example\n",
    "\n",
    "Flux is the Julia Machine Learning. The website comes with plenty of resources: https://fluxml.ai/\n",
    "Let's jump to functional code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTuple{Matrix{Float32}, Matrix{Float32}}[([256.0 2100.0 4096.0 512.0 515.0 5500.0 8192.0], [435867.0 2.959963f6 1.475489f6 1.485569f6 2.592234f6 4.51803f6])]\n",
      "Initial solution: Float32[5620.0 42500.0 82420.0 10740.0 10800.0 110500.0 164340.0]\n"
     ]
    },
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: arrays could not be broadcast to a common size: a has axes Base.OneTo(7) and b has axes Base.OneTo(6)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: arrays could not be broadcast to a common size: a has axes Base.OneTo(7) and b has axes Base.OneTo(6)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] _bcs1\n",
      "    @ ./broadcast.jl:528 [inlined]\n",
      "  [2] _bcs (repeats 2 times)\n",
      "    @ ./broadcast.jl:522 [inlined]\n",
      "  [3] broadcast_shape\n",
      "    @ ./broadcast.jl:516 [inlined]\n",
      "  [4] combine_axes\n",
      "    @ ./broadcast.jl:497 [inlined]\n",
      "  [5] instantiate\n",
      "    @ ./broadcast.jl:307 [inlined]\n",
      "  [6] materialize\n",
      "    @ ./broadcast.jl:872 [inlined]\n",
      "  [7] adjoint\n",
      "    @ ~/.julia/packages/Zygote/wfLOG/src/lib/broadcast.jl:86 [inlined]\n",
      "  [8] _pullback\n",
      "    @ ~/.julia/packages/ZygoteRules/CkVIK/src/adjoint.jl:67 [inlined]\n",
      "  [9] loss\n",
      "    @ ~/workspace/julia-basics/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W1sZmlsZQ==.jl:21 [inlined]\n",
      " [10] _pullback(::Zygote.Context{false}, ::var\"#loss#14\", ::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, ::Matrix{Float32}, ::Matrix{Float32})\n",
      "    @ Zygote ~/.julia/packages/Zygote/wfLOG/src/compiler/interface2.jl:0\n",
      " [11] _apply\n",
      "    @ ./boot.jl:946 [inlined]\n",
      " [12] adjoint\n",
      "    @ ~/.julia/packages/Zygote/wfLOG/src/lib/lib.jl:199 [inlined]\n",
      " [13] _pullback\n",
      "    @ ~/.julia/packages/ZygoteRules/CkVIK/src/adjoint.jl:67 [inlined]\n",
      " [14] #4\n",
      "    @ ~/.julia/packages/Flux/3711C/src/train.jl:117 [inlined]\n",
      " [15] _pullback(ctx::Zygote.Context{false}, f::Flux.Train.var\"#4#5\"{var\"#loss#14\", Tuple{Matrix{Float32}, Matrix{Float32}}}, args::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/wfLOG/src/compiler/interface2.jl:0\n",
      " [16] pullback(f::Function, cx::Zygote.Context{false}, args::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/wfLOG/src/compiler/interface.jl:96\n",
      " [17] pullback\n",
      "    @ ~/.julia/packages/Zygote/wfLOG/src/compiler/interface.jl:94 [inlined]\n",
      " [18] withgradient(f::Function, args::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/wfLOG/src/compiler/interface.jl:211\n",
      " [19] macro expansion\n",
      "    @ ~/.julia/packages/Flux/3711C/src/train.jl:117 [inlined]\n",
      " [20] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]\n",
      " [21] train!(loss::Function, model::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}; cb::Nothing)\n",
      "    @ Flux.Train ~/.julia/packages/Flux/3711C/src/train.jl:114\n",
      " [22] train!(loss::Function, model::Flux.Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, data::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, opt::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}})\n",
      "    @ Flux.Train ~/.julia/packages/Flux/3711C/src/train.jl:111\n",
      " [23] _flux_linear_fit(nepocs::Int64, X::Vector{Float32}, Y::Vector{Float32}, W0::Float64, b0::Float64, plot::Bool)\n",
      "    @ Main ~/workspace/julia-basics/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W1sZmlsZQ==.jl:38\n",
      " [24] test()\n",
      "    @ Main ~/workspace/julia-basics/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W1sZmlsZQ==.jl:78\n",
      " [25] top-level scope\n",
      "    @ ~/workspace/julia-basics/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W1sZmlsZQ==.jl:81"
     ]
    }
   ],
   "source": [
    "import Flux\n",
    "import Statistics\n",
    "\n",
    "import Plots;\n",
    "using LaTeXStrings\n",
    "\n",
    "function _flux_linear_fit(nepocs, X, Y, W0, b0, plot)\n",
    "    # Prepare the training data (horizontal concatenation)\n",
    "    Xd = reduce(hcat, X)\n",
    "    Yd = reduce(hcat, Y)\n",
    "    data = [(Xd, Yd)]\n",
    "    println(\"Data\", data)\n",
    "\n",
    "    # Define the model. Weight and bias are arrays initialized to some values.\n",
    "    # Training a model will adjust weights and biases.\n",
    "    model = Flux.Dense(1, 1) # model = W x + b\n",
    "    model.weight .= [W0]\n",
    "    model.bias .= [b0]\n",
    "\n",
    "    # Define the loss function\n",
    "    loss(model, x, y) = Statistics.mean((model(x) .- y) .^ 2)\n",
    "\n",
    "    # Define an optimizer\n",
    "    # optimizer = Flux.Descent(1)\n",
    "    # optimizer = Flux.ADAM()\n",
    "    optimizer = Flux.ADAM(1, (0.99, 0.999))\n",
    "    opt = Flux.setup(optimizer, model)\n",
    "\n",
    "    Yd_0 = model(Xd)\n",
    "    println(\"Initial solution: \", Yd_0)\n",
    "\n",
    "    # Plot the initial data\n",
    "    if plot\n",
    "        Plots.plot(X, Y, st=:scatter, label=\"y\", legend=:topleft)\n",
    "    end\n",
    "\n",
    "    for iter = 1:nepocs\n",
    "        Flux.train!(loss, model, data, opt)\n",
    "\n",
    "        # Plot the evolution of the model\n",
    "        if iter % 10 == 0\n",
    "            if plot\n",
    "                if iter <= 100\n",
    "                    Yd_nE = model(Xd)\n",
    "                    Plots.plot!(Xd', Yd_nE', lc=:blue, label=L\"y_{%$iter}\")\n",
    "                elseif 100 < iter && iter <= 1000 && iter % 200 == 0\n",
    "                    Yd_nE = model(Xd)\n",
    "                    Plots.plot!(Xd', Yd_nE', lc=:orange, label=L\"y_{%$iter}\")\n",
    "                elseif 1000 < iter && iter <= nepocs && iter % 500 == 0\n",
    "                    Yd_nE = model(Xd)\n",
    "                    Plots.plot!(Xd', Yd_nE', lc=:red, label=L\"y_{%$iter}\")\n",
    "                end\n",
    "            end\n",
    "            println(\"Epoch: \", iter, \" Loss: \", loss(model, Xd, Yd))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Yd_nE = model(Xd)\n",
    "    println(\"Final solution: \", Yd_nE)\n",
    "\n",
    "    if plot\n",
    "        Plots.plot!(Xd', Yd_0', lc=:green, label=L\"y_0\")\n",
    "        Plots.plot!(xlabel=\"x\", ylabel=\"y\", title=\"Flux.jl Linear Fit\")\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "function test()\n",
    "    x = Float32[    256,     2_100,     4_096,       512,     5_500,     8_192]\n",
    "    y = Float32[435_867, 2_959_963, 1_475_489, 1_485_569, 2_592_234, 4_518_030]\n",
    "\n",
    "    W0 = 20.0\n",
    "    b0 = 500.0\n",
    "\n",
    "    nepocs = 2_000\n",
    "    plot = true\n",
    "\n",
    "    _flux_linear_fit(nepocs, x, y, W0, b0, plot)\n",
    "end\n",
    "\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
